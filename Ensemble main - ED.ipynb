{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c69f70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_signature():\n",
    "    print(\"=\"*40)\n",
    "    print(\"START OF NOTEBOOK — AT\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "start_signature()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddcc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# lets look at the base table - transfers \n",
    "df_transfers = pd.read_csv(r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\transfers.csv')\n",
    "df_transfers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915fd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to know the unique number of patients and unique admissions \n",
    "\n",
    "unique_patients = df_transfers[\"subject_id\"].nunique()\n",
    "unique_admissions = df_transfers[\"hadm_id\"].nunique()\n",
    "unique_transfers = df_transfers[\"transfer_id\"].nunique()\n",
    "print(\"the no of unique patients :\", unique_patients)\n",
    "print(\" the no of unique admissions :\", unique_admissions)\n",
    "print(\" the no of unique transfers :\", unique_transfers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7df95d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d973b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am gonna calculate our output variable now - time spent in ED. Now for that first I need only ED data , everything else not needed so need to remove that\n",
    "# Secondly remove any blank values and then create a new variable called LOS_ED - length of stay in ED\n",
    "# Goal is to predict how long a patient will stay in the ED during a single hospital visit, so we will  use the admission level.\n",
    "# LOS_ED_houurs - defined as the time spent by a patient in emergency department calculated by the difference between intime and outtime \n",
    "\n",
    "units = ['Emergency Department','Emergency Department Observation']\n",
    "df_transfers = df_transfers[df_transfers['careunit'].isin(units)].copy()\n",
    "# df_transfers = df_transfers.dropna(subset =['intime','outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a596489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549c36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets keep datetime format and calculate LOS_ED (hours)\n",
    "df_transfers['intime'] = pd.to_datetime(df_transfers['intime'])\n",
    "df_transfers['outtime'] = pd.to_datetime(df_transfers['outtime'])\n",
    "\n",
    "df_transfers['LOS_ED'] = (df_transfers['outtime']-df_transfers['intime']).dt.total_seconds()/3600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedb138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets aggregate at admission id level, need to sum the times spent in Emergency department and emergency department observation\n",
    "# if a patient had multiple ED transfers hence why.\n",
    "# keeping subject_id , so that if i want later can keep the count of patients \n",
    "\n",
    "df_transfers = df_transfers.groupby('hadm_id',as_index= False).agg({\n",
    "    'LOS_ED':'sum',\n",
    "    'subject_id':'first'}).rename(columns={'LOS_ED':'LOS_ED_hours'})\n",
    "\n",
    "df_transfers['LOS_ED_hours']=df_transfers['LOS_ED_hours'].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0737a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_transfers.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c968513",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers.info()\n",
    "#df_transfers.isnull().any().any()\n",
    "df_transfers.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658dbee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['LOS_ED_hours'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8b85dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df_transfers['LOS_ED_hours'], bins=400, kde=True, color='blue')\n",
    "plt.title('Distribution of ED Length of Stay (hours) with Density Curve')\n",
    "plt.xlabel('LOS_ED_hours')\n",
    "plt.ylabel('Density / Count')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a549ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['LOS_ED_hours'].describe(percentiles=[0.25, 0.5, 0.75, 0.90, 0.95, 0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa7e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=df_transfers['LOS_ED_hours'], color='lightblue')\n",
    "plt.title('Boxplot of Target Variable')\n",
    "plt.xlabel('Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d95a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_transfers['LOS_ED_hours'].quantile(0.25)\n",
    "Q3 = df_transfers['LOS_ED_hours'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "\n",
    "outliers = df_transfers[(df_transfers['LOS_ED_hours'] > upper_limit) | (df_transfers['LOS_ED_hours'] < lower_limit)]\n",
    "print(f\"Outlier count: {len(outliers)} ({len(outliers)/len(df_transfers)*100:.2f}% of total)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e9d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df_transfers['LOS_ED_hours'].quantile(0.75) + 1.5 * (df_transfers['LOS_ED_hours'].quantile(0.75) - df_transfers['LOS_ED_hours'].quantile(0.25))\n",
    "print(\"Upper limit (IQR method):\", upper_limit)\n",
    "\n",
    "df_transfers.loc[df_transfers['LOS_ED_hours'] > upper_limit, 'LOS_ED_hours'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa7746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "# Original\n",
    "sns.histplot(df_transfers['LOS_ED_hours'], kde=True, bins=80, color='lightblue', stat='count', ax=axes[0])\n",
    "axes[0].set_xlim(0, df_transfers['LOS_ED_hours'].quantile(0.99))\n",
    "axes[0].set_title('Original Data')\n",
    "\n",
    "# Without outliers\n",
    "sns.histplot(df_transfers.loc[df_transfers['LOS_ED_hours'] <= upper_limit, 'LOS_ED_hours'], kde=True, bins=80, color='salmon', stat='count', ax=axes[1])\n",
    "axes[1].set_xlim(0, df_transfers['LOS_ED_hours'].quantile(0.99))\n",
    "axes[1].set_title('Without Outliers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e77cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['is_outlier'] = df_transfers['LOS_ED_hours'] > 24\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df_transfers, x='LOS_ED_hours', hue='is_outlier', bins=100, stat='density',\n",
    "             palette={False: 'skyblue', True: 'salmon'}, kde=True)\n",
    "plt.xlim(0, df_transfers['LOS_ED_hours'].quantile(0.99))\n",
    "plt.title('Distribution — Highlighting Outliers (>24)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3252bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "sns.histplot(df_transfers['LOS_ED_hours'], kde=True, bins=80, stat='count', color='skyblue', ax=axes[0])\n",
    "axes[0].set_xlim(0, df_transfers['LOS_ED_hours'].quantile(0.99))\n",
    "axes[0].set_title('Raw Target (0–99th percentile)')\n",
    "\n",
    "sns.histplot(np.log1p(df_transfers['LOS_ED_hours']), kde=True, bins=80, stat='count', color='coral', ax=axes[1])\n",
    "axes[1].set_title('Log(1+Target)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f545492",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = pd.read_csv(r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\patients.csv')\n",
    "df_patients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a67c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_patients['anchor_age'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Patient Age')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Density / Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='gender', data=df_patients, palette='pastel')\n",
    "plt.title('Distribution of Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d509baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets keep only required columns in patients table\n",
    "df_patients = df_patients.drop(columns=['anchor_year','anchor_year_group','dod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ff8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1=df_transfers.merge(df_patients,on='subject_id',how='left')\n",
    "df_merge1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987b3e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1=df_merge1[df_merge1['anchor_age'] >=65]\n",
    "df_merge1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0457ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e529b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.regplot(x='anchor_age', y='LOS_ED_hours', data=df_merge1, scatter_kws={'alpha':0.3}, line_kws={'color':'red'})\n",
    "plt.title('ED LOS vs Age with Trend Line')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('LOS_ED_hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_merge1['anchor_age'].corr(df_merge1['LOS_ED_hours'])\n",
    "print(f\"Correlation between age and ED LOS: {corr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb29a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(x='gender', y='LOS_ED_hours', data=df_merge1, palette='viridis')\n",
    "plt.title('ED LOS by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('LOS_ED_hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = pd.read_csv(r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\admissions.csv')\n",
    "df_admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843bbb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b195e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.drop(columns=['dischtime','discharge_location','language',\n",
    "                                            'edregtime','edouttime','hospital_expire_flag','deathtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac91eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.dropna(subset=['marital_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb3a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66106328",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32905de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets understand about provider\n",
    "\n",
    "df_admissions['admit_provider_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81104e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_per_doctor = df_admissions.groupby('admit_provider_id')['subject_id'].nunique().reset_index()\n",
    "patients_per_doctor.rename(columns={'subject_id': 'unique_patients'}, inplace=True)\n",
    "\n",
    "print(patients_per_doctor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581f1be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='admission_type', data=df_admissions, palette='Set2')\n",
    "plt.title('Type of Admission - Distribution', pad=15)   \n",
    "plt.xlabel('Admission_Type')\n",
    "plt.ylabel('No of Patients')\n",
    "\n",
    "# trying to rotate axis coz the titles were overlapping here\n",
    "plt.xticks(rotation=15, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228ffaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='admission_location', data=df_admissions, palette='Set2')\n",
    "plt.title(' Admission Location - Distribution', pad=20)   \n",
    "plt.xlabel('Admission_Location')\n",
    "plt.ylabel('No of Patients')\n",
    "\n",
    "# trying to rotate axis coz the titles were overlapping here\n",
    "plt.xticks(rotation=25, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fced10",
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_counts = df_admissions['insurance'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(\n",
    "    insurance_counts, \n",
    "    labels=insurance_counts.index, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=90, \n",
    "    colors=plt.cm.Pastel1.colors\n",
    ")\n",
    "plt.title('Insurance Type Distribution', pad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03d66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2 = pd.merge(df_merge1, df_admissions, on=['hadm_id','subject_id'], how='inner')\n",
    "#using inner here instead of left coz i need matching records from both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079a709",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455a0adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y='insurance', x='LOS_ED_hours', data=df_merge2, palette='Set2')\n",
    "plt.title('ED LOS by Insurance Type', pad=15)\n",
    "plt.xlabel('LOS_ED_hours')\n",
    "plt.ylabel('Insurance Type')\n",
    "plt.xticks(rotation=30)  # Rotate labels if too long\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa82378",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y='admission_type', x='LOS_ED_hours', data=df_merge2, palette='Set2')\n",
    "plt.title('ED LOS by Admission Type', pad=15)\n",
    "plt.xlabel('LOS_ED_hours')\n",
    "plt.ylabel('Admission Type')\n",
    "plt.xticks(rotation=30)  # Rotate labels if too long\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4978cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2['LOS_ED_hours'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_los_race = df_merge2.groupby('race')['LOS_ED_hours'].mean().reset_index()\n",
    "mean_los_race = mean_los_race.sort_values(by='LOS_ED_hours', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    y='race', \n",
    "    x='LOS_ED_hours', \n",
    "    data=mean_los_race,\n",
    "    palette='pastel'\n",
    ")\n",
    "plt.title('Average ED LOS by Race (Sorted)', pad=15)\n",
    "plt.xlabel('Average LOS_ED_hours')\n",
    "plt.ylabel('Race')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7265141",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_los_race = df_merge2.groupby('marital_status')['LOS_ED_hours'].mean().reset_index()\n",
    "mean_los_race = mean_los_race.sort_values(by='LOS_ED_hours', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    y='marital_status', \n",
    "    x='LOS_ED_hours', \n",
    "    data=mean_los_race,\n",
    "    palette='viridis'\n",
    ")\n",
    "plt.title('Average ED LOS by marital_Status (Sorted)', pad=15)\n",
    "plt.xlabel('Average LOS_ED_hours')\n",
    "plt.ylabel('marital_status')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a06d3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_los_race = df_merge2.groupby('admission_location')['LOS_ED_hours'].mean().reset_index()\n",
    "mean_los_race = mean_los_race.sort_values(by='LOS_ED_hours', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "sns.barplot(\n",
    "    y='admission_location', \n",
    "    x='LOS_ED_hours', \n",
    "    data=mean_los_race,\n",
    "    palette='Blues'\n",
    ")\n",
    "plt.title('Average ED LOS by admission_location (Sorted)', pad=15)\n",
    "plt.xlabel('Average LOS_ED_hours')\n",
    "plt.ylabel('admission_location')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcb1326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2['admittime'] = pd.to_datetime(df_merge2['admittime'], errors='coerce')\n",
    "df_merge2['admit_hour'] = df_merge2['admittime'].dt.hour\n",
    "df_merge2['admit_day'] = df_merge2['admittime'].dt.dayofweek  # 0=Monday, 6=Sunday\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f252cbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x='admit_hour', y='LOS_ED_hours', data=df_merge2, palette='pastel')\n",
    "plt.title('ED LOS by Hour of Admission', pad=15)\n",
    "plt.xlabel('Hour of Admission')\n",
    "plt.ylabel('LOS_ED_hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f11f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x='admit_day', y='LOS_ED_hours', data=df_merge2, palette='Set2')\n",
    "plt.title('ED LOS by Day of Week', pad=15)\n",
    "plt.xlabel('Day of Week (0=Mon)')\n",
    "plt.ylabel('LOS_ED_hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953cb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(\n",
    "    x='admit_hour',\n",
    "    y='LOS_ED_hours',\n",
    "    data=df_merge2,\n",
    "    color='skyblue',\n",
    "    alpha=0.4,\n",
    "    s=50\n",
    ")\n",
    "\n",
    "sns.regplot(\n",
    "    x='admit_hour',\n",
    "    y='LOS_ED_hours',\n",
    "    data=df_merge2,\n",
    "    scatter=False,          \n",
    "    color='red',\n",
    "    line_kws={'lw':2}\n",
    ")\n",
    "\n",
    "plt.title('ED LOS by Hour of Admission with Trend', pad=15)\n",
    "plt.xlabel('Hour of Admission (0=Midnight)')\n",
    "plt.ylabel('LOS_ED_hours')\n",
    "plt.xticks(range(0,24))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48237b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\diagnoses_icd.csv.gz'\n",
    "df_diagnoses = pd.read_csv(file_path, compression ='gzip')\n",
    "df_diagnoses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6199f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique ICD9 codes in this dataset.'.format(df_diagnoses['icd_code'].value_counts().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245db57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_icd9_short(code):\n",
    "    \"\"\"Return short ICD-9 category name\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return 'misc'\n",
    "    \n",
    "    code_str = str(code).strip().upper()\n",
    "    if code_str.startswith(('E', 'V')):\n",
    "        return 'misc'  \n",
    "    \n",
    "    try:\n",
    "        num = int(code_str[:3])\n",
    "    except ValueError:\n",
    "        return 'misc'\n",
    "    \n",
    "    ranges = [\n",
    "        ((1, 139), 'infectious'),\n",
    "        ((140, 239), 'neoplasms'),\n",
    "        ((240, 279), 'endocrine'),\n",
    "        ((280, 289), 'blood'),\n",
    "        ((290, 319), 'mental'),\n",
    "        ((320, 389), 'nervous'),\n",
    "        ((390, 459), 'circulatory'),\n",
    "        ((460, 519), 'respiratory'),\n",
    "        ((520, 579), 'digestive'),\n",
    "        ((580, 629), 'genitourinary'),\n",
    "        ((630, 679), 'pregnancy'),\n",
    "        ((680, 709), 'skin'),\n",
    "        ((710, 739), 'muscular'),\n",
    "        ((740, 759), 'congenital'),\n",
    "        ((760, 779), 'prenatal'),\n",
    "        ((780, 799), 'misc'),\n",
    "        ((800, 999), 'injury')\n",
    "    ]\n",
    "    \n",
    "    for (low, high), label in ranges:\n",
    "        if low <= num <= high:\n",
    "            return label\n",
    "    return 'misc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431fb9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses['icd_category'] = df_diagnoses['icd_code'].apply(categorize_icd9_short)\n",
    "\n",
    "diag_counts = (\n",
    "    df_diagnoses.groupby(['hadm_id', 'icd_category'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c667cbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2 = df_merge2.merge(diag_counts, on='hadm_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2260cf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merge2.fillna(0, inplace=True)\n",
    "\n",
    "for col in diag_counts.columns:\n",
    "    if col != 'hadm_id':\n",
    "        df_merge2[col] = df_merge2[col].astype(int)\n",
    "\n",
    "\n",
    "print(df_merge2.shape)\n",
    "df_merge2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b42c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e124d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "diseases = [\n",
    "    'blood', 'circulatory', 'congenital', 'digestive', 'endocrine',\n",
    "    'genitourinary', 'infectious', 'injury', 'mental', 'misc',\n",
    "    'muscular', 'neoplasms', 'nervous', 'respiratory', 'skin'\n",
    "]\n",
    "\n",
    "mean_los = {}\n",
    "for disease in diseases:\n",
    "    mean_los[disease] = df_merge2.loc[df_merge2[disease] > 0, 'LOS_ED_hours'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "bars = sns.barplot(x=list(mean_los.keys()), y=list(mean_los.values()), palette='viridis')\n",
    "\n",
    "\n",
    "for bar in bars.patches:\n",
    "    height = bar.get_height()\n",
    "    bars.annotate(f'{height:.2f}',  # 2 decimal places\n",
    "                  xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                  xytext=(0, 5),  \n",
    "                  textcoords='offset points',\n",
    "                  ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Mean LOS in ED (hours)')\n",
    "plt.xlabel('Disease Category')\n",
    "plt.title('Mean ED Length of Stay by Disease Category')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c098190",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826b49d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18893ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53202663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "\n",
    "X = df_merge2.drop(columns=[\"LOS_ED_hours\", \"hadm_id\", \"subject_id\"], errors=\"ignore\").copy()\n",
    "y = df_merge2[\"LOS_ED_hours\"].astype(float).copy()\n",
    "\n",
    "\n",
    "if (y < 0).any():\n",
    "    raise ValueError(\"Found negative LOS_ED_hours values. Fix data before log transform.\")\n",
    "\n",
    "\n",
    "\n",
    "datetime_cols = X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "\n",
    "for col in datetime_cols:\n",
    "    dt = pd.to_datetime(X[col], errors=\"coerce\")\n",
    "    X[col + \"_hour\"] = dt.dt.hour\n",
    "    X[col + \"_weekday\"] = dt.dt.weekday\n",
    "    X[col + \"_month\"] = dt.dt.month\n",
    "\n",
    "X.drop(columns=datetime_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "cat_like = X.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns\n",
    "X[cat_like] = X[cat_like].astype(str)\n",
    "\n",
    "\n",
    "X[cat_like] = X[cat_like].replace({\"nan\": np.nan, \"NaT\": np.nan, \"None\": np.nan})\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ac61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log = np.log1p(y_test)\n",
    "\n",
    "\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6377b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=50,      \n",
    "    max_depth=10,         \n",
    "    min_samples_leaf=5,   \n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "et = ExtraTreesRegressor(\n",
    "    n_estimators=80,      \n",
    "    max_depth=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "gbr_stack = GradientBoostingRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ridge = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "voting = VotingRegressor(\n",
    "    estimators=[(\"et\", et), (\"rf\", rf), (\"ridge\", ridge),(\"gbr_stack\", gbr_stack)]\n",
    ")\n",
    "\n",
    "# Weighted voting (ET strongest)\n",
    "weighted_voting = VotingRegressor(\n",
    "    estimators=[(\"et\", et), (\"rf\", rf), (\"ridge\", ridge),(\"gbr_stack\", gbr_stack)],\n",
    "    weights=[2, 1, 0.5,3]\n",
    ")\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": rf,\n",
    "    \"ExtraTrees\": et,\n",
    "    \"GradientBoostingRegressor\":gbr_stack,\n",
    "    \"VotingRegressor\": voting,\n",
    "    \"WeightedVotingRegressor\": weighted_voting\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a32f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    pipe.fit(X_train, y_train_log)\n",
    "\n",
    "    y_pred_log = pipe.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "    \n",
    "    y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2_hours = r2_score(y_test, y_pred)\n",
    "    r2_log = r2_score(y_test_log, y_pred_log)\n",
    "\n",
    "    results[name] = {\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"MSE\": mse,\n",
    "        \"R2_hours\": r2_hours,\n",
    "        \"R2_log\": r2_log\n",
    "    }\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(\"MAE\")\n",
    "print(\"\\n===== RESULTS (sorted by MAE) =====\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a83c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMPLE STACKING - same models \n",
    "\n",
    "\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "rf_stack = RandomForestRegressor(\n",
    "    n_estimators=120,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "et_stack = ExtraTreesRegressor(\n",
    "    n_estimators=180,\n",
    "    max_depth=15,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "meta = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "\n",
    "simple_stacking = StackingRegressor(\n",
    "    estimators=[(\"rf\", rf_stack), (\"et\", et_stack)],\n",
    "    final_estimator=meta,\n",
    "    passthrough=False,\n",
    "    cv=3,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "\n",
    "stack_pipe = Pipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", simple_stacking)\n",
    "])\n",
    "\n",
    "print(\"\\nTraining: SimpleStacking\")\n",
    "stack_pipe.fit(X_train, y_train_log)\n",
    "\n",
    "y_pred_log_stack = stack_pipe.predict(X_test)\n",
    "y_pred_stack = np.expm1(y_pred_log_stack)\n",
    "y_pred_stack = np.clip(y_pred_stack, 0, None)\n",
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred_stack)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred_stack)\n",
    "r2_hours = r2_score(y_test, y_pred_stack)\n",
    "r2_log = r2_score(y_test_log, y_pred_log_stack)\n",
    "\n",
    "\n",
    "results[\"SimpleStacking\"] = {\n",
    "    \"MAE\": mae,\n",
    "    \"RMSE\": rmse,\n",
    "    \"MSE\": mse,\n",
    "    \"R2_hours\": r2_hours,\n",
    "    \"R2_log\": r2_log\n",
    "}\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(\"MAE\")\n",
    "print(\"\\n===== RESULTS (INCLUDING STACKING) =====\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835bc7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## the long computation time code -- because of cv folds---\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# from sklearn.ensemble import (\n",
    "#     RandomForestRegressor,\n",
    "#     ExtraTreesRegressor,\n",
    "#     GradientBoostingRegressor,\n",
    "#     AdaBoostRegressor,\n",
    "#     HistGradientBoostingRegressor,\n",
    "#     StackingRegressor,\n",
    "#     VotingRegressor\n",
    "# )\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "\n",
    "# X = df_merge2.drop(columns=[\"LOS_ED_hours\", \"hadm_id\", \"subject_id\"], errors=\"ignore\").copy()\n",
    "# y = df_merge2[\"LOS_ED_hours\"].astype(float).copy()\n",
    "\n",
    "\n",
    "# if (y < 0).any():\n",
    "#     raise ValueError(\"Found negative LOS_ED_hours values. Fix data before log transform.\")\n",
    "\n",
    "\n",
    "\n",
    "# datetime_cols = X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "\n",
    "# for col in datetime_cols:\n",
    "#     dt = pd.to_datetime(X[col], errors=\"coerce\")\n",
    "#     X[col + \"_hour\"] = dt.dt.hour\n",
    "#     X[col + \"_weekday\"] = dt.dt.weekday\n",
    "#     X[col + \"_month\"] = dt.dt.month\n",
    "\n",
    "# X.drop(columns=datetime_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# cat_like = X.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns\n",
    "# X[cat_like] = X[cat_like].astype(str)\n",
    "\n",
    "# X[cat_like] = X[cat_like].replace({\n",
    "#     \"nan\": np.nan,\n",
    "#     \"NaT\": np.nan,\n",
    "#     \"None\": np.nan\n",
    "# })\n",
    "\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=42\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# y_train_log = np.log1p(y_train)\n",
    "# y_test_log = np.log1p(y_test)\n",
    "\n",
    "\n",
    "\n",
    "# num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# cat_cols = [c for c in X_train.columns if c not in num_cols]\n",
    "\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"median\"))\n",
    "# ])\n",
    "\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "#     (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "# ])\n",
    "\n",
    "# preprocess = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         (\"num\", numeric_transformer, num_cols),\n",
    "#         (\"cat\", categorical_transformer, cat_cols)\n",
    "#     ],\n",
    "#     remainder=\"drop\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# ensemble_models = {\n",
    "#     \"RandomForest\": RandomForestRegressor(\n",
    "#         n_estimators=300,\n",
    "#         random_state=42,\n",
    "#         n_jobs=4\n",
    "#     ),\n",
    "#     \"ExtraTrees\": ExtraTreesRegressor(\n",
    "#         n_estimators=300,\n",
    "#         random_state=42,\n",
    "#         n_jobs=4\n",
    "#     ),\n",
    "#     \"GradientBoosting\": GradientBoostingRegressor(\n",
    "#         n_estimators=300,\n",
    "#         random_state=42\n",
    "#     ),\n",
    "#     \"AdaBoost\": AdaBoostRegressor(\n",
    "#         n_estimators=400,\n",
    "#         random_state=42\n",
    "#     ),\n",
    "#     \"HistGradientBoosting\": HistGradientBoostingRegressor(\n",
    "#         random_state=42\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# voting = VotingRegressor(\n",
    "#     estimators=[\n",
    "#         (\"rf\", ensemble_models[\"RandomForest\"]),\n",
    "#         (\"et\", ensemble_models[\"ExtraTrees\"]),\n",
    "#         (\"hgb\", ensemble_models[\"HistGradientBoosting\"])\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# stacking = StackingRegressor(\n",
    "#     estimators=[\n",
    "#         (\"rf\", ensemble_models[\"RandomForest\"]),\n",
    "#         (\"et\", ensemble_models[\"ExtraTrees\"]),\n",
    "#         (\"gbr\", ensemble_models[\"GradientBoosting\"]),\n",
    "#         (\"hgb\", ensemble_models[\"HistGradientBoosting\"])\n",
    "#     ],\n",
    "#     final_estimator=Ridge(alpha=1.0),\n",
    "#     passthrough=False,\n",
    "#     cv=3,\n",
    "#     n_jobs=1\n",
    "# )\n",
    "\n",
    "# ensemble_models[\"VotingRegressor\"] = voting\n",
    "# ensemble_models[\"StackingRegressor\"] = stacking\n",
    "\n",
    "\n",
    "\n",
    "# results = {}\n",
    "\n",
    "# for name, model in ensemble_models.items():\n",
    "#     pipe = Pipeline(steps=[\n",
    "#         (\"preprocess\", preprocess),\n",
    "#         (\"model\", model)\n",
    "#     ])\n",
    "\n",
    "#     pipe.fit(X_train, y_train_log)\n",
    "\n",
    "#     y_pred_log = pipe.predict(X_test)\n",
    "#     y_pred = np.expm1(y_pred_log)\n",
    "#     y_pred = np.clip(y_pred, 0, None)\n",
    "\n",
    "#     mse = mean_squared_error(y_test, y_pred)\n",
    "#     rmse = np.sqrt(mse)\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     r2_hours = r2_score(y_test, y_pred)\n",
    "#     r2_log = r2_score(y_test_log, y_pred_log)\n",
    "\n",
    "#     results[name] = {\n",
    "#         \"MAE\": mae,\n",
    "#         \"RMSE\": rmse,\n",
    "#         \"MSE\": mse,\n",
    "#         \"R2_hours\": r2_hours,\n",
    "#         \"R2_log\": r2_log\n",
    "#     }\n",
    "\n",
    "# results_df = pd.DataFrame(results).T.sort_values(\"MAE\")\n",
    "# print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_signature():\n",
    "    print(\"=\"*40)\n",
    "    print(\"END OF NOTEBOOK — AT\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "end_signature()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
