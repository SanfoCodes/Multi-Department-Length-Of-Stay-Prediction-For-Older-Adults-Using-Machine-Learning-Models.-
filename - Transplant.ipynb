{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba15e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_signature():\n",
    "    print(\"=\"*40)\n",
    "    print(\"START OF NOTEBOOK — AT\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "start_signature()\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae0d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# lets look at the base table - transfers \n",
    "df_transfers = pd.read_csv(r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\transfers.csv')\n",
    "df_transfers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fd762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to know the unique number of patients and unique admissions \n",
    "\n",
    "print(\"the no of unique patients :\", df_transfers[\"subject_id\"].nunique())\n",
    "print(\" the no of unique admissions :\", df_transfers[\"hadm_id\"].nunique())\n",
    "print(\" the no of unique transfers :\", df_transfers[\"transfer_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93cf0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am gonna calculate our output variable now - time spent in ED. Now for that first I need only ED data , everything else not needed so need to remove that\n",
    "# Secondly remove any blank values and then create a new variable called LOS_ED - length of stay in ED\n",
    "# Goal is to predict how long a patient will stay in the ED during a single hospital visit, so we will  use the admission level.\n",
    "# LOS_ED_houurs - defined as the time spent by a patient in emergency department calculated by the difference between intime and outtime \n",
    "\n",
    "units = ['Transplant']\n",
    "df_transfers = df_transfers[df_transfers['careunit'].isin(units)].copy()\n",
    "# df_transfers = df_transfers.dropna(subset =['intime','outtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6c35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['intime'] = pd.to_datetime(df_transfers['intime'], errors='coerce')\n",
    "df_transfers['outtime'] = pd.to_datetime(df_transfers['outtime'], errors='coerce')\n",
    "\n",
    "df_transfers = df_transfers.dropna(subset=['intime','outtime'])\n",
    "df_transfers = df_transfers[df_transfers['outtime'] >= df_transfers['intime']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets keep datetime format and calculate LOS_HC (hours)\n",
    "df_transfers['LOS_Transplant'] = (df_transfers['outtime']-df_transfers['intime']).dt.total_seconds()/3600\n",
    "df_transfers = df_transfers[df_transfers[\"LOS_Transplant\"] > 0].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd7590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets aggregate at admission id level, need to sum the times spent in Emergency department and emergency department observation\n",
    "# if a patient had multiple ED transfers hence why.\n",
    "# keeping subject_id , so that if i want later can keep the count of patients \n",
    "\n",
    "df_transfers = (\n",
    "    df_transfers.groupby(\"hadm_id\", as_index=False)\n",
    "    .agg({\"LOS_Transplant\": \"sum\", \"subject_id\": \"first\"})\n",
    "    .rename(columns={\"LOS_Transplant\": \"LOS_Transplant_hours\"})\n",
    ")\n",
    "\n",
    "df_transfers[\"LOS_Transplant_hours\"] = df_transfers[\"LOS_Transplant_hours\"].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98270176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (df_transfers.head())\n",
    "print(\"Transplant admissions:\", df_transfers[\"hadm_id\"].nunique())\n",
    "print(\"Transplant patients:\", df_transfers[\"subject_id\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers.info()\n",
    "#df_transfers.isnull().any().any()\n",
    "df_transfers.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710491ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['LOS_Transplant_hours'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(df_transfers['LOS_Transplant_hours'], bins=400, kde=True, color='blue')\n",
    "plt.title('Distribution of \tTransplant Length of Stay (hours) with Density Curve')\n",
    "plt.xlabel('LOS_Transplant_hours')\n",
    "plt.ylabel('Density / Count')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['LOS_Transplant_hours'].describe(percentiles=[0.25, 0.5, 0.75, 0.90, 0.95, 0.99])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f2eb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "sns.boxplot(x=df_transfers['LOS_Transplant_hours'], color='lightblue')\n",
    "plt.title('Boxplot of Target Variable')\n",
    "plt.xlabel('Target')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a751251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df_transfers['LOS_Transplant_hours'].quantile(0.25)\n",
    "Q3 = df_transfers['LOS_Transplant_hours'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "upper_limit = Q3 + 1.5 * IQR\n",
    "lower_limit = Q1 - 1.5 * IQR\n",
    "\n",
    "outliers = df_transfers[(df_transfers['LOS_Transplant_hours'] > upper_limit) | (df_transfers['LOS_Transplant_hours'] < lower_limit)]\n",
    "print(f\"Outlier count: {len(outliers)} ({len(outliers)/len(df_transfers)*100:.2f}% of total)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d244bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_limit = df_transfers['LOS_Transplant_hours'].quantile(0.75) + 1.5 * (df_transfers['LOS_Transplant_hours'].quantile(0.75) - df_transfers['LOS_Transplant_hours'].quantile(0.25))\n",
    "print(\"Upper limit (IQR method):\", upper_limit)\n",
    "\n",
    "df_transfers.loc[df_transfers['LOS_Transplant_hours'] > upper_limit, 'LOS_Transplant_hours'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3b687",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "sns.histplot(df_transfers[\"LOS_Transplant_hours\"], kde=True, bins=80, color=\"lightblue\", stat=\"count\", ax=axes[0])\n",
    "axes[0].set_xlim(0, df_transfers[\"LOS_Transplant_hours\"].quantile(0.99))\n",
    "axes[0].set_title(\"Original Data (0–99th percentile)\")\n",
    "\n",
    "sns.histplot(df_transfers.loc[df_transfers[\"LOS_Transplant_hours\"] <= upper_limit, \"LOS_Transplant_hours\"],\n",
    "             kde=True, bins=80, color=\"salmon\", stat=\"count\", ax=axes[1])\n",
    "axes[1].set_xlim(0, df_transfers[\"LOS_Transplant_hours\"].quantile(0.99))\n",
    "axes[1].set_title(\"Without IQR Outliers (0–99th percentile)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73789ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transfers['is_outlier'] = df_transfers['LOS_Transplant_hours'] > 24\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(data=df_transfers, x='LOS_Transplant_hours', hue='is_outlier', bins=100, stat='density',\n",
    "             palette={False: 'skyblue', True: 'salmon'}, kde=True)\n",
    "plt.xlim(0, df_transfers['LOS_Transplant_hours'].quantile(0.99))\n",
    "plt.title('Distribution — Highlighting Outliers (>24)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4885c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14,6))\n",
    "\n",
    "sns.histplot(df_transfers['LOS_Transplant_hours'], kde=True, bins=80, stat='count', color='skyblue', ax=axes[0])\n",
    "axes[0].set_xlim(0, df_transfers['LOS_Transplant_hours'].quantile(0.99))\n",
    "axes[0].set_title('Raw Target (0–99th percentile)')\n",
    "\n",
    "sns.histplot(np.log1p(df_transfers['LOS_Transplant_hours']), kde=True, bins=80, stat='count', color='coral', ax=axes[1])\n",
    "axes[1].set_title('Log(1+Target)')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d44056",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients = pd.read_csv(r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\patients.csv')\n",
    "df_patients.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccda03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f8d297",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_patients.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64293c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.histplot(df_patients['anchor_age'], bins=30, kde=True, color='skyblue')\n",
    "plt.title('Distribution of Patient Age')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('Density / Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9f3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='gender', data=df_patients, palette='pastel')\n",
    "plt.title('Distribution of Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa61d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets keep only required columns in patients table\n",
    "df_patients = df_patients.drop(columns=['anchor_year','anchor_year_group','dod'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1=df_transfers.merge(df_patients,on='subject_id',how='left')\n",
    "df_merge1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85b949",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09278c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge1=df_merge1[df_merge1['anchor_age'] >=65].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merge1.shape)\n",
    "print(df_merge1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979c12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.regplot(x='anchor_age', y='LOS_Transplant_hours', data=df_merge1, scatter_kws={'alpha':0.3}, line_kws={'color':'red'})\n",
    "plt.title(' Transplant vs Age with Trend Line')\n",
    "plt.xlabel('Age (years)')\n",
    "plt.ylabel('LOS_Transplant_hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30325646",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_merge1['anchor_age'].corr(df_merge1['LOS_Transplant_hours'])\n",
    "print(f\"Correlation between age and Transplant LOS: {corr:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e3b01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "sns.boxplot(x='gender', y='LOS_Transplant_hours', data=df_merge1, palette='viridis')\n",
    "plt.title('Transplant LOS by Gender')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('LOS_Transplant_hours')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88fe63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = pd.read_csv(r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\admissions.csv')\n",
    "df_admissions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3c4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ce2038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep admittime and drop leakage columns\n",
    "df_admissions[\"admittime\"] = pd.to_datetime(df_admissions[\"admittime\"], errors=\"coerce\")\n",
    "\n",
    "df_admissions = df_admissions.drop(columns=['dischtime','discharge_location','language',\n",
    "                                            'edregtime','edouttime','hospital_expire_flag','deathtime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions = df_admissions.dropna(subset=['marital_status'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fac530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3beead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_admissions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bbf0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets understand about provider\n",
    "\n",
    "df_admissions['admit_provider_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6329bb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_per_doctor = df_admissions.groupby('admit_provider_id')['subject_id'].nunique().reset_index()\n",
    "patients_per_doctor.rename(columns={'subject_id': 'unique_patients'}, inplace=True)\n",
    "\n",
    "print(patients_per_doctor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3743edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2 = pd.merge(df_merge1, df_admissions, on=['hadm_id','subject_id'], how='inner')\n",
    "print(df_merge2.shape)\n",
    "#using inner here instead of left coz i need matching records from both tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_merge2.shape)\n",
    "\n",
    "# admission time features\n",
    "df_merge2[\"admit_hour\"]  = df_merge2[\"admittime\"].dt.hour\n",
    "df_merge2[\"admit_day\"]   = df_merge2[\"admittime\"].dt.dayofweek\n",
    "df_merge2[\"admit_month\"] = df_merge2[\"admittime\"].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ece046",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='admission_type', data=df_admissions, palette='Set2')\n",
    "plt.title('Type of Admission - Distribution', pad=15)   \n",
    "plt.xlabel('Admission_Type')\n",
    "plt.ylabel('No of Patients')\n",
    "\n",
    "# trying to rotate axis coz the titles were overlapping here\n",
    "plt.xticks(rotation=15, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f52336",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='admission_location', data=df_admissions, palette='Set2')\n",
    "plt.title(' Admission Location - Distribution', pad=20)   \n",
    "plt.xlabel('Admission_Location')\n",
    "plt.ylabel('No of Patients')\n",
    "\n",
    "# trying to rotate axis coz the titles were overlapping here\n",
    "plt.xticks(rotation=25, ha='right') \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bc8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "insurance_counts = df_admissions['insurance'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.pie(\n",
    "    insurance_counts, \n",
    "    labels=insurance_counts.index, \n",
    "    autopct='%1.1f%%', \n",
    "    startangle=90, \n",
    "    colors=plt.cm.Pastel1.colors\n",
    ")\n",
    "plt.title('Insurance Type Distribution', pad=20)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884cf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y='insurance', x='LOS_Transplant_hours', data=df_merge2, palette='Set2')\n",
    "plt.title('Transplant LOS by Insurance Type', pad=15)\n",
    "plt.xlabel('LOS_Transplant_hours')\n",
    "plt.ylabel('Insurance Type')\n",
    "plt.xticks(rotation=30)  # Rotate labels if too long\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2915f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(y='admission_type', x='LOS_Transplant_hours', data=df_merge2, palette='Set2')\n",
    "plt.title('Transplant LOS by Admission Type', pad=15)\n",
    "plt.xlabel('LOS_Transplant_hours')\n",
    "plt.ylabel('Admission Type')\n",
    "plt.xticks(rotation=30)  # Rotate labels if too long\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55100bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# admit hour/day EDA\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x=\"admit_hour\", y=\"LOS_Transplant_hours\", data=df_merge2)\n",
    "plt.title(\"Transplant LOS by Hour of Admission\", pad=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbd8e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "sns.boxplot(x=\"admit_day\", y=\"LOS_Transplant_hours\", data=df_merge2)\n",
    "plt.title(\"Transplant LOS by Day of Week\", pad=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cb4ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\diagnoses_icd.csv.gz'\n",
    "df_diagnoses = pd.read_csv(file_path, compression ='gzip')\n",
    "df_diagnoses.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5b38da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} unique ICD9 codes in this dataset.'.format(df_diagnoses['icd_code'].value_counts().count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27018e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_icd9_short(code):\n",
    "    \"\"\"Return short ICD-9 category name\"\"\"\n",
    "    if pd.isna(code):\n",
    "        return 'misc'\n",
    "    \n",
    "    code_str = str(code).strip().upper()\n",
    "    if code_str.startswith(('E', 'V')):\n",
    "        return 'misc'  \n",
    "    \n",
    "    try:\n",
    "        num = int(code_str[:3])\n",
    "    except ValueError:\n",
    "        return 'misc'\n",
    "    \n",
    "    ranges = [\n",
    "        ((1, 139), 'infectious'),\n",
    "        ((140, 239), 'neoplasms'),\n",
    "        ((240, 279), 'endocrine'),\n",
    "        ((280, 289), 'blood'),\n",
    "        ((290, 319), 'mental'),\n",
    "        ((320, 389), 'nervous'),\n",
    "        ((390, 459), 'circulatory'),\n",
    "        ((460, 519), 'respiratory'),\n",
    "        ((520, 579), 'digestive'),\n",
    "        ((580, 629), 'genitourinary'),\n",
    "        ((630, 679), 'pregnancy'),\n",
    "        ((680, 709), 'skin'),\n",
    "        ((710, 739), 'muscular'),\n",
    "        ((740, 759), 'congenital'),\n",
    "        ((760, 779), 'prenatal'),\n",
    "        ((780, 799), 'misc'),\n",
    "        ((800, 999), 'injury')\n",
    "    ]\n",
    "    \n",
    "    for (low, high), label in ranges:\n",
    "        if low <= num <= high:\n",
    "            return label\n",
    "    return 'misc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42503e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diagnoses['icd_category'] = df_diagnoses['icd_code'].apply(categorize_icd9_short)\n",
    "\n",
    "diag_counts = (\n",
    "    df_diagnoses.groupby(['hadm_id', 'icd_category'])\n",
    "    .size()\n",
    "    .unstack(fill_value=0)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c583610",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2 = df_merge2.merge(diag_counts, on=\"hadm_id\", how=\"left\").fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e209310",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merge2.fillna(0, inplace=True)\n",
    "\n",
    "for col in diag_counts.columns:\n",
    "    if col != 'hadm_id':\n",
    "        df_merge2[col] = df_merge2[col].astype(int)\n",
    "\n",
    "\n",
    "print(df_merge2.shape)\n",
    "df_merge2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff10e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diag = df_diagnoses[[\"hadm_id\", \"icd_code\", \"icd_version\"]].copy()\n",
    "\n",
    "def normalize_icd(code):\n",
    "    if pd.isna(code):\n",
    "        return \"\"\n",
    "    return str(code).strip().upper().replace(\".\", \"\")\n",
    "\n",
    "df_diag[\"icd_code_clean\"] = df_diag[\"icd_code\"].apply(normalize_icd)\n",
    "df_diag[\"is_icd10\"] = (df_diag[\"icd_version\"] == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97df1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def icd3(row):\n",
    "    c = row[\"icd_code_clean\"]\n",
    "    if c == \"\":\n",
    "        return \"\"\n",
    "    if row[\"is_icd10\"]:\n",
    "        return c[:3]  # ICD-10 prefix\n",
    "    digits = \"\".join(ch for ch in c if ch.isdigit())\n",
    "    return digits[:3] if len(digits) >= 3 else digits\n",
    "\n",
    "df_diag[\"icd3\"] = df_diag.apply(icd3, axis=1)\n",
    "\n",
    "# better complexity features\n",
    "diag_complex = (\n",
    "    df_diag.groupby(\"hadm_id\")\n",
    "    .agg(\n",
    "        n_icd_codes=(\"icd_code_clean\", \"nunique\"),\n",
    "        n_unique_icd3=(\"icd3\", lambda x: x.nunique())\n",
    "    )\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c552e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transplant-focused ICD flags\n",
    "ICD10_PREFIX_TX = {\n",
    "    \"has_transplant_status\": {\"Z94\"},\n",
    "    \"has_transplant_complication\": {\"T86\"},\n",
    "    \"has_ckd\": {\"N18\"},\n",
    "    \"has_aki\": {\"N17\"},\n",
    "    \"has_liver_disease\": {\"K70\",\"K71\",\"K72\",\"K73\",\"K74\",\"K76\"},\n",
    "    \"has_diabetes\": {\"E10\",\"E11\"},\n",
    "    \"has_htn\": {\"I10\",\"I11\",\"I12\",\"I13\"},\n",
    "    \"has_sepsis\": {\"A41\"},\n",
    "}\n",
    "\n",
    "ICD9_RANGES_TX = {\n",
    "    \"has_transplant_complication\": [(996, 996)],  # 996.xx\n",
    "    \"has_ckd\": [(585, 585)],\n",
    "    \"has_aki\": [(584, 584)],\n",
    "    \"has_liver_disease\": [(570, 573)],\n",
    "    \"has_diabetes\": [(250, 250)],\n",
    "    \"has_htn\": [(401, 405)],\n",
    "    \"has_sepsis\": [(995, 995)],  # rough\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122b9152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_icd10(icd3_list, prefixes):\n",
    "    return int(any(str(v) in prefixes for v in icd3_list if str(v)))\n",
    "\n",
    "def flag_icd9(icd3_list, ranges):\n",
    "    nums = [int(v) for v in icd3_list if str(v).isdigit()]\n",
    "    for lo, hi in ranges:\n",
    "        if any(lo <= n <= hi for n in nums):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "flags_rows = []\n",
    "for hadm_id, g in df_diag.groupby(\"hadm_id\"):\n",
    "    icd3_list = g[\"icd3\"].dropna().tolist()\n",
    "    out = {\"hadm_id\": hadm_id}\n",
    "\n",
    "    for feat, prefset in ICD10_PREFIX_TX.items():\n",
    "        out[feat] = flag_icd10(icd3_list, prefset)\n",
    "\n",
    "    for feat, ranges in ICD9_RANGES_TX.items():\n",
    "        out[feat] = max(out.get(feat, 0), flag_icd9(icd3_list, ranges))\n",
    "\n",
    "    flags_rows.append(out)\n",
    "\n",
    "diag_flags = pd.DataFrame(flags_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd3804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge2 = df_merge2.merge(diag_complex, on=\"hadm_id\", how=\"left\")\n",
    "df_merge2 = df_merge2.merge(diag_flags, on=\"hadm_id\", how=\"left\")\n",
    "\n",
    "new_cols = [\"n_icd_codes\", \"n_unique_icd3\"] + list(ICD10_PREFIX_TX.keys())\n",
    "for c in new_cols:\n",
    "    if c in df_merge2.columns:\n",
    "        df_merge2[c] = df_merge2[c].fillna(0).astype(float)\n",
    "\n",
    "# extra combined feature\n",
    "df_merge2[\"any_transplant_related_dx\"] = (\n",
    "    (df_merge2[\"has_transplant_status\"] > 0) |\n",
    "    (df_merge2[\"has_transplant_complication\"] > 0)\n",
    ").astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211400f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✅ Added transplant dx features:\", new_cols + [\"any_transplant_related_dx\"])\n",
    "print(\"df_merge2 shape:\", df_merge2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f5af63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_hadm_ids = set(df_merge2[\"hadm_id\"].dropna().unique())\n",
    "print(\"Transplant admissions:\", len(tx_hadm_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5141f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DLABITEMS_PATH = r\"C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\d_labitems.csv\"\n",
    "df_labitems = pd.read_csv(DLABITEMS_PATH, usecols=[\"itemid\", \"label\"])\n",
    "\n",
    "TRANSPLANT_LABS = {\n",
    "    \"Creatinine\", \"Urea Nitrogen\", \"Glucose\",\n",
    "    \"Sodium\", \"Potassium\", \"Chloride\", \"Bicarbonate\",\n",
    "    \"White Blood Cells\", \"Hemoglobin\", \"Platelet Count\",\n",
    "    \"Bilirubin, Total\", \"AST (SGOT)\", \"ALT (SGPT)\", \"Alkaline Phosphatase\",\n",
    "    \"INR(PT)\", \"PT\", \"PTT\"\n",
    "}\n",
    "\n",
    "tx_labitems = df_labitems[df_labitems[\"label\"].isin(TRANSPLANT_LABS)].copy()\n",
    "tx_lab_itemids = set(tx_labitems[\"itemid\"].unique())\n",
    "\n",
    "print(\"Selected transplant lab labels found:\")\n",
    "print(tx_labitems[\"label\"].value_counts())\n",
    "print(\"ItemIDs to keep:\", tx_lab_itemids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5134cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_time_map = (\n",
    "    df_admissions[[\"hadm_id\", \"admittime\"]]\n",
    "    .assign(admittime=lambda x: pd.to_datetime(x[\"admittime\"], errors=\"coerce\"))\n",
    "    .set_index(\"hadm_id\")[\"admittime\"]\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b9768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "LABEVENTS_PATH = r\"C:\\Users\\arpitha_work\\Downloads\\TRU MSCDS\\Sem 3\\Graduate Project\\MIMIC\\mimic-iv-3.1\\hosp\\labevents.csv.gz\"\n",
    "\n",
    "EARLY_WINDOW_HOURS = 12\n",
    "CHUNKSIZE = 50_000\n",
    "\n",
    "t0 = time.time()\n",
    "chunk_counter = 0\n",
    "rows_kept_total = 0\n",
    "\n",
    "sum_dict   = defaultdict(float)\n",
    "count_dict = defaultdict(int)\n",
    "min_dict   = defaultdict(lambda: np.inf)\n",
    "max_dict   = defaultdict(lambda: -np.inf)\n",
    "\n",
    "for chunk in pd.read_csv(\n",
    "    LABEVENTS_PATH,\n",
    "    compression=\"gzip\",\n",
    "    usecols=[\"hadm_id\", \"itemid\", \"charttime\", \"valuenum\"],\n",
    "    chunksize=CHUNKSIZE\n",
    "):\n",
    "    chunk_counter += 1\n",
    "\n",
    "    chunk = chunk[\n",
    "        chunk[\"hadm_id\"].isin(tx_hadm_ids) &\n",
    "        chunk[\"itemid\"].isin(tx_lab_itemids)\n",
    "    ].dropna(subset=[\"charttime\", \"valuenum\"])\n",
    "\n",
    "    if chunk.empty:\n",
    "        if chunk_counter % 50 == 0:\n",
    "            print(f\"chunks={chunk_counter}, kept_rows={rows_kept_total}, elapsed={(time.time()-t0)/60:.1f} min\")\n",
    "        continue\n",
    "\n",
    "    chunk[\"charttime\"] = pd.to_datetime(chunk[\"charttime\"], errors=\"coerce\")\n",
    "    chunk = chunk.dropna(subset=[\"charttime\"])\n",
    "\n",
    "    chunk[\"admittime\"] = chunk[\"hadm_id\"].map(adm_time_map)\n",
    "    chunk = chunk.dropna(subset=[\"admittime\"])\n",
    "\n",
    "    hours_from_admit = (chunk[\"charttime\"] - chunk[\"admittime\"]).dt.total_seconds() / 3600.0\n",
    "    chunk = chunk[(hours_from_admit >= 0) & (hours_from_admit <= EARLY_WINDOW_HOURS)]\n",
    "\n",
    "    if chunk.empty:\n",
    "        if chunk_counter % 50 == 0:\n",
    "            print(f\"chunks={chunk_counter}, kept_rows={rows_kept_total}, elapsed={(time.time()-t0)/60:.1f} min\")\n",
    "        continue\n",
    "\n",
    "    rows_kept_total += len(chunk)\n",
    "\n",
    "    g = chunk.groupby([\"hadm_id\", \"itemid\"])[\"valuenum\"]\n",
    "    s = g.sum()\n",
    "    c = g.count()\n",
    "    mn = g.min()\n",
    "    mx = g.max()\n",
    "\n",
    "    for k, v in s.items():\n",
    "        sum_dict[k] += float(v)\n",
    "    for k, v in c.items():\n",
    "        count_dict[k] += int(v)\n",
    "    for k, v in mn.items():\n",
    "        min_dict[k] = min(min_dict[k], float(v))\n",
    "    for k, v in mx.items():\n",
    "        max_dict[k] = max(max_dict[k], float(v))\n",
    "\n",
    "    if chunk_counter % 50 == 0:\n",
    "        print(f\"chunks={chunk_counter}, kept_rows={rows_kept_total}, elapsed={(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "print(\"Finished streaming labevents\")\n",
    "print(\"Total chunks:\", chunk_counter)\n",
    "print(\"Rows kept:\", rows_kept_total)\n",
    "print(\"Total minutes:\", (time.time() - t0) / 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for (hadm_id, itemid), cnt in count_dict.items():\n",
    "    rows.append({\n",
    "        \"hadm_id\": hadm_id,\n",
    "        \"itemid\": itemid,\n",
    "        \"lab_mean\": sum_dict[(hadm_id, itemid)] / cnt,\n",
    "        \"lab_min\":  min_dict[(hadm_id, itemid)],\n",
    "        \"lab_max\":  max_dict[(hadm_id, itemid)],\n",
    "        \"lab_count\": cnt\n",
    "    })\n",
    "\n",
    "lab_agg = pd.DataFrame(rows)\n",
    "lab_agg = lab_agg.merge(df_labitems, on=\"itemid\", how=\"left\")\n",
    "\n",
    "print(\"\\nLab long table:\", lab_agg.shape)\n",
    "print(lab_agg.head())\n",
    "\n",
    "lab_agg.to_csv(\"tx_lab_agg_long.csv\", index=False)\n",
    "print(\"Saved: tx_lab_agg_long.csv\")\n",
    "\n",
    "lab_agg[\"lab_sum\"] = lab_agg[\"lab_mean\"] * lab_agg[\"lab_count\"]\n",
    "\n",
    "lab_collapsed = (\n",
    "    lab_agg\n",
    "    .groupby([\"hadm_id\", \"label\"], as_index=False)\n",
    "    .agg(\n",
    "        lab_sum=(\"lab_sum\", \"sum\"),\n",
    "        lab_count=(\"lab_count\", \"sum\"),\n",
    "        lab_min=(\"lab_min\", \"min\"),\n",
    "        lab_max=(\"lab_max\", \"max\")\n",
    "    )\n",
    ")\n",
    "lab_collapsed[\"lab_mean\"] = lab_collapsed[\"lab_sum\"] / lab_collapsed[\"lab_count\"]\n",
    "\n",
    "lab_wide = lab_collapsed.pivot_table(\n",
    "    index=\"hadm_id\",\n",
    "    columns=\"label\",\n",
    "    values=[\"lab_mean\",\"lab_min\",\"lab_max\",\"lab_count\"],\n",
    "    aggfunc=\"first\"\n",
    ")\n",
    "lab_wide.columns = [f\"{stat}_{label}\".replace(\" \", \"_\") for stat, label in lab_wide.columns]\n",
    "lab_wide = lab_wide.reset_index()\n",
    "\n",
    "print(\"\\nLab wide features:\", lab_wide.shape)\n",
    "print(lab_wide.head())\n",
    "\n",
    "df_merge2 = df_merge2.merge(lab_wide, on=\"hadm_id\", how=\"left\")\n",
    "print(\"\\n✅ df_merge2 shape after merging labs:\", df_merge2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cf858",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------------------\n",
    " #DERIVED TRANSPLANT LAB FEATURES \n",
    "# ------------------------------\n",
    "import numpy as np\n",
    "\n",
    "def safe_div(a, b):\n",
    "    return np.where((b == 0) | pd.isna(b), np.nan, a / b)\n",
    "\n",
    "# Liver injury signal\n",
    "if \"lab_mean_AST_(SGOT)\" in df_merge2.columns and \"lab_mean_ALT_(SGPT)\" in df_merge2.columns:\n",
    "    df_merge2[\"ast_alt_ratio\"] = safe_div(df_merge2[\"lab_mean_AST_(SGOT)\"], df_merge2[\"lab_mean_ALT_(SGPT)\"])\n",
    "\n",
    "# Renal dysfunction signal\n",
    "if \"lab_mean_Creatinine\" in df_merge2.columns and \"lab_mean_Urea_Nitrogen\" in df_merge2.columns:\n",
    "    df_merge2[\"bun_creatinine_ratio\"] = safe_div(df_merge2[\"lab_mean_Urea_Nitrogen\"], df_merge2[\"lab_mean_Creatinine\"])\n",
    "\n",
    "# Coagulation severity proxy\n",
    "if \"lab_mean_INR(PT)\" in df_merge2.columns:\n",
    "    df_merge2[\"inr_high_flag\"] = (df_merge2[\"lab_mean_INR(PT)\"] >= 1.5).astype(int)\n",
    "\n",
    "print(\"✅ Added derived lab features (if available):\",\n",
    "      [c for c in [\"ast_alt_ratio\",\"bun_creatinine_ratio\",\"inr_high_flag\"] if c in df_merge2.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "adm = df_admissions[[\"hadm_id\",\"subject_id\",\"admittime\"]].copy()\n",
    "adm[\"admittime\"] = pd.to_datetime(adm[\"admittime\"], errors=\"coerce\")\n",
    "adm = adm.sort_values([\"subject_id\",\"admittime\"]).copy()\n",
    "\n",
    "adm[\"prev_adm_count\"] = adm.groupby(\"subject_id\").cumcount()\n",
    "adm[\"prev_admittime\"] = adm.groupby(\"subject_id\")[\"admittime\"].shift(1)\n",
    "\n",
    "adm[\"days_since_last_adm\"] = (adm[\"admittime\"] - adm[\"prev_admittime\"]).dt.total_seconds() / (3600*24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_feats = adm[[\"hadm_id\",\"prev_adm_count\",\"days_since_last_adm\"]].copy()\n",
    "\n",
    "df_merge2 = df_merge2.merge(hist_feats, on=\"hadm_id\", how=\"left\")\n",
    "df_merge2[\"prev_adm_count\"] = df_merge2[\"prev_adm_count\"].fillna(0).astype(float)\n",
    "df_merge2[\"days_since_last_adm\"] = df_merge2[\"days_since_last_adm\"].fillna(-1).astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b37168",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(df_merge2[[\"prev_adm_count\",\"days_since_last_adm\"]].describe())\n",
    "print(\"df_merge2 shape:\", df_merge2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855a2ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COL = \"LOS_Transplant_hours\"\n",
    "\n",
    "df_merge2 = df_merge2[df_merge2[TARGET_COL].notna()].copy()\n",
    "df_merge2 = df_merge2[df_merge2[TARGET_COL] > 0].copy()\n",
    "\n",
    "# Quantile-based bins (3 classes) — safer for transplant\n",
    "q1, q2 = df_merge2[TARGET_COL].quantile([0.33, 0.66])\n",
    "bins = [0, q1, q2, np.inf]\n",
    "labels = [0, 1, 2]  # 0=short, 1=medium, 2=long\n",
    "\n",
    "df_merge2[\"los_class\"] = pd.cut(df_merge2[TARGET_COL], bins=bins, labels=labels, right=False)\n",
    "df_merge2 = df_merge2[df_merge2[\"los_class\"].notna()].copy()\n",
    "df_merge2[\"los_class\"] = df_merge2[\"los_class\"].astype(int)\n",
    "\n",
    "print(\"Bins used:\", bins)\n",
    "print(df_merge2[\"los_class\"].value_counts(normalize=True).sort_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f080591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, balanced_accuracy_score\n",
    "\n",
    "X = df_merge2.drop(columns=[TARGET_COL, \"los_class\", \"hadm_id\", \"subject_id\"], errors=\"ignore\").copy()\n",
    "y = df_merge2[\"los_class\"].astype(int).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52d096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datetime -> engineered time parts\n",
    "datetime_cols = X.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns.tolist()\n",
    "for col in datetime_cols:\n",
    "    dt = pd.to_datetime(X[col], errors=\"coerce\")\n",
    "    X[col + \"_hour\"] = dt.dt.hour\n",
    "    X[col + \"_weekday\"] = dt.dt.weekday\n",
    "    X[col + \"_month\"] = dt.dt.month\n",
    "X.drop(columns=datetime_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# categorical cleanup\n",
    "cat_like = X.select_dtypes(include=[\"object\", \"string\", \"category\"]).columns\n",
    "X[cat_like] = X[cat_like].astype(str)\n",
    "X[cat_like] = X[cat_like].replace({\"nan\": np.nan, \"NaT\": np.nan, \"None\": np.nan})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07e8c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ patient-level split (prevents leakage)\n",
    "groups = df_merge2[\"subject_id\"]\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_test = X.iloc[train_idx].copy(), X.iloc[test_idx].copy()\n",
    "y_train, y_test = y.iloc[train_idx].copy(), y.iloc[test_idx].copy()\n",
    "\n",
    "num_cols = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = [c for c in X_train.columns if c not in num_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8c4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, num_cols),\n",
    "        (\"cat\", categorical_transformer, cat_cols)\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    sparse_threshold=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b56c551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=200, max_depth=12, min_samples_leaf=5,\n",
    "    random_state=42, n_jobs=1, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "et = ExtraTreesClassifier(\n",
    "    n_estimators=300, max_depth=12, min_samples_leaf=5,\n",
    "    random_state=42, n_jobs=1, class_weight=\"balanced\"\n",
    ")\n",
    "\n",
    "gbr = GradientBoostingClassifier(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=3, random_state=42\n",
    ")\n",
    "\n",
    "logreg = LogisticRegression(max_iter=2000, class_weight=\"balanced\")\n",
    "\n",
    "voting_soft = VotingClassifier(\n",
    "    estimators=[(\"lr\", logreg), (\"rf\", rf), (\"et\", et), (\"gbr\", gbr)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "models = {\n",
    "    \"LogisticRegression\": logreg,\n",
    "    \"RandomForest\": rf,\n",
    "    \"ExtraTrees\": et,\n",
    "    \"GradientBoostingClassifier\": gbr,\n",
    "    \"VotingSoft\": voting_soft\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffda22",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline(steps=[(\"preprocess\", preprocess), (\"model\", model)])\n",
    "\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "    pipe.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = pipe.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "\n",
    "    results[name] = {\n",
    "        \"Accuracy\": acc,\n",
    "        \"Balanced_Acc\": bal_acc,\n",
    "        \"F1_macro\": f1_macro,\n",
    "        \"F1_weighted\": f1_weighted\n",
    "    }\n",
    "\n",
    "    print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "results_df = pd.DataFrame(results).T.sort_values(\"F1_macro\", ascending=False)\n",
    "print(\"\\n===== RESULTS (sorted by F1_macro) =====\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8911f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20a9cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed3fb0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78691704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b6205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ea4061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ed24b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9829c4f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_signature():\n",
    "    print(\"=\"*40)\n",
    "    print(\"END OF NOTEBOOK — AT\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "end_signature()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mimic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
